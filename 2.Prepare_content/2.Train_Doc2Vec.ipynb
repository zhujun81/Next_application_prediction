{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d1e4569-2772-45a2-9562-55a58b5c23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993c2e3d-62db-4c87-b735-6503fc0b3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../data/\"\n",
    "dataset = \"cb12/\"\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfc6f7-ea3d-4177-a960-e67d5c63f055",
   "metadata": {},
   "source": [
    "# Step 1: Load job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2df22b6-41a8-4dd2-b07b-ffc6d1414957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading job from file: ../data/cb12/processed/jobs_14d_30_consider_user_encoded_tokenized\n",
      "Job data shape:  (207972, 27)\n",
      "Unique JobCity:  5744\n",
      "Unique JobState:  54\n",
      "Unique JobCountry:  3\n"
     ]
    }
   ],
   "source": [
    "print('Loading job from file: {}'.format(processed_path + 'jobs_14d_30_consider_user_encoded_tokenized'))\n",
    "job_df_30 = pd.read_csv(processed_path + 'jobs_14d_30_consider_user_encoded_tokenized.csv', header=0, sep='\\t')\n",
    "print('Job data shape: ', job_df_30.shape)\n",
    "print('Unique JobCity: ', len(job_df_30.JobCity.unique()))\n",
    "print('Unique JobState: ', len(job_df_30.JobState.unique()))\n",
    "print('Unique JobCountry: ', len(job_df_30.JobCountry.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f20d7-32a3-422d-a6e9-5196445234bf",
   "metadata": {},
   "source": [
    "# Step 2: Get pre-tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde6b96f-6183-454b-a849-8b1f74c34361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f3b81e-d471-49b4-bc64-964bc26bbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_freq(tokenized_texts):\n",
    "    words_freq = FreqDist([word for text in tokenized_texts for word in text])\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1085f4a-f6bc-4a57-88bd-85743517d08f",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cd24ba4-75c5-49d2-86e6-2e7d54bf6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word frequencies...\n",
      "Number of vocabulary in Title (raw): 19929\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [eval(t) for t in job_df_30['Title_tokenized'].values.tolist()]\n",
    "print('Computing word frequencies...')\n",
    "words_freq= get_words_freq(tokenized_texts)\n",
    "print('Number of vocabulary in {} (raw): {}'.format('Title', len(words_freq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6277a2c-ee11-4ee1-b9c2-bc0906bc7ecc",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a769056-041a-49d3-9ff6-5b40612aaaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word frequencies...\n",
      "Number of vocabulary in All (raw): 820935\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_all = [eval(t) for t in job_df_30['All_tokenized'].values.tolist()]\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_all = get_words_freq(tokenized_texts_all)\n",
    "print('Number of vocabulary in {} (raw): {}'.format('All', len(words_freq_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f9c3a-66ed-4865-b363-fb3c06ef70bb",
   "metadata": {},
   "source": [
    "# Step 3: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ca6cd5-0e98-48df-8885-f0a2ece21063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e416ae1-e06f-4485-81a4-7e3bc17300bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents...\n"
     ]
    }
   ],
   "source": [
    "print('Processing documents...')\n",
    "tagged_data = [TaggedDocument(words=w, tags=[i]) for i, w in enumerate(tokenized_texts_all)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2887b228-3f0f-46c1-9484-49de19c4545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training doc2vec\n"
     ]
    }
   ],
   "source": [
    "print('Training doc2vec')\n",
    "max_epochs = 30\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "model = Doc2Vec(\n",
    "    vector_size=vec_size,\n",
    "    alpha=alpha, \n",
    "    min_alpha=alpha,   \n",
    "    window=5,\n",
    "    negative=5,\n",
    "    min_count=2,                                     \n",
    "    max_vocab_size=100000,\n",
    "    dm = 1,\n",
    "    dm_mean=1,\n",
    "    workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a85beb-b08c-4a60-adcd-ad2472dc5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269eb47e-0fa4-4457-a97b-f43264921c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=1) \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "del tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d9d1385-8279-46f8-9566-0d2e63d5c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating job content embeddings, making sure that they are sorted by the encoded JobID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('Concatenating job content embeddings, making sure that they are sorted by the encoded JobID')\n",
    "job_content_embeddings = np.vstack([model.docvecs[i-1] for i in job_df_30['JobID_encoded'].values])    \n",
    "embedding_for_padding_job = np.mean(job_content_embeddings, axis=0)\n",
    "content_job_embeddings_with_padding = np.vstack([embedding_for_padding_job, job_content_embeddings])\n",
    "del job_content_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dbdcd1a-cd4d-4f75-b838-2b74f9a40b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(filename, obj):\n",
    "    with tf.io.gfile.GFile(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "\n",
    "def export_job_content_embeddings(content_job_embeddings, output_job_content_embeddings_path):\n",
    "    output_path = output_job_content_embeddings_path\n",
    "    print('Exporting job embeddings to {}'.format(output_path))\n",
    "    #to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)\n",
    "    to_serialize = content_job_embeddings\n",
    "    serialize(output_path, to_serialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2327a77-4cea-40c9-849b-585a590f6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting job content embeddings\n",
      "Exporting job embeddings to ../language_models/pickles/jobs_14d_30_consider_user_All_d2v.pickle\n"
     ]
    }
   ],
   "source": [
    "#Checking if content job embedding size correspond to the last JobID\n",
    "assert content_job_embeddings_with_padding.shape[0] == job_df_30['JobID_encoded'].tail(1).values[0]+1\n",
    "print('Exporting job content embeddings')\n",
    "del job_df_30\n",
    "export_job_content_embeddings(content_job_embeddings_with_padding, '../language_models/pickles/jobs_14d_30_consider_user_All_d2v.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c1d84-838e-4bad-b325-f32dc221d41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
