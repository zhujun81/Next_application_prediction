{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c539918a-efb6-499a-a12f-3daea1a817a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87774920-bf9f-4cc4-85f2-148e02e70890",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../data/\"\n",
    "dataset = \"cb12/\"\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2721ba-da7c-4bd0-ab10-a65cd6f98d67",
   "metadata": {},
   "source": [
    "# Step 1: Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71dd9036-c187-4c9e-9f4e-319ac8557298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "def load_word_embeddings(path, binary=True):\n",
    "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "    return w2v_model\n",
    "\n",
    "w2v_model = load_word_embeddings('', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c61cdf-520e-4ede-b76c-1fce4a2d2e4d",
   "metadata": {},
   "source": [
    "# Step 2: Load job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04008d3-7482-4d52-949a-2efb7b51dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading job from file: ../data/cb12/processed/jobs_14d_30_consider_user_encoded_tokenized\n",
      "Job data shape:  (207972, 27)\n",
      "Unique JobCity:  5744\n",
      "Unique JobState:  54\n",
      "Unique JobCountry:  3\n"
     ]
    }
   ],
   "source": [
    "print('Loading job from file: {}'.format(processed_path + 'jobs_14d_30_consider_user_encoded_tokenized'))\n",
    "job_df_30 = pd.read_csv(processed_path + 'jobs_14d_30_consider_user_encoded_tokenized.csv', header=0, sep='\\t')\n",
    "print('Job data shape: ', job_df_30.shape)\n",
    "print('Unique JobCity: ', len(job_df_30.JobCity.unique()))\n",
    "print('Unique JobState: ', len(job_df_30.JobState.unique()))\n",
    "print('Unique JobCountry: ', len(job_df_30.JobCountry.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b26f9-b5ce-44b4-9b18-f8b971af4a7b",
   "metadata": {},
   "source": [
    "# Step 3: Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa862464-0912-41f6-8611-15c7b3a0b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f7a810-f376-4642-bd5f-67f9727a6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_freq(tokenized_texts):\n",
    "    words_freq = FreqDist([word for text in tokenized_texts for word in text])\n",
    "    return words_freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "313cd057-be11-4607-9ab6-bfefc57ffcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "def google_process_word_embedding_for_corpus_vocab(w2v_model, words_freq, keep_most_frequent_words=100000):\n",
    "    print('Tokens vocab. from texts: {}'.format(len(words_freq)))\n",
    "    if len(words_freq) < keep_most_frequent_words:\n",
    "        most_freq_words = []\n",
    "        for word, freq in words_freq.items():\n",
    "            if freq > 1:\n",
    "                most_freq_words.append(word)\n",
    "        print('Tokens vocab. from texts with freq > 1: {}'.format(len(most_freq_words)))\n",
    "\n",
    "    else:\n",
    "        most_freq_words = []\n",
    "        for word, freq in words_freq.items():\n",
    "            if freq > 5:\n",
    "                most_freq_words.append(word)\n",
    "        print('Tokens vocab. from texts with freq > 1: {}'.format(len(most_freq_words)))\n",
    "\n",
    "\n",
    "        #most_freq_words = set(list(map(lambda x: x[0], words_freq.most_common(keep_most_frequent_words))))\n",
    "        print('Most common tokens vocab. from texts: {}'.format(len(most_freq_words)))\n",
    "\n",
    "    RESERVED_TOKENS_IN_VOCAB=2\n",
    "    embedding_size = w2v_model.vector_size\n",
    "    new_embeddings_list = []\n",
    "    new_vocab = {}\n",
    "\n",
    "    last_token_id = RESERVED_TOKENS_IN_VOCAB\n",
    "    w2v_vocab = set(w2v_model.index_to_key)\n",
    "    for word in list(most_freq_words):        \n",
    "        if word in list(w2v_vocab):    \n",
    "            new_vocab[word] = last_token_id\n",
    "            last_token_id += 1\n",
    "            new_embeddings_list.append(w2v_model[word])\n",
    "\n",
    "    #Inserting the 2 reserved tokens\n",
    "    new_vocab[PAD_TOKEN] = 0\n",
    "    new_vocab[UNK_TOKEN] = 1\n",
    "\n",
    "    np.random.seed(10)\n",
    "    unk_vector = np.random.uniform(low=-0.04, high=0.04, size=embedding_size)\n",
    "    pad_vector = np.random.uniform(low=-0.04, high=0.04, size=embedding_size)\n",
    "\n",
    "    new_embeddings_matrix = np.vstack([unk_vector, pad_vector] + new_embeddings_list)\n",
    "\n",
    "    print('Most common tokens with word embeddings: {}'.format(new_embeddings_matrix.shape[0]))\n",
    "    return new_vocab, new_embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e2524f-7e75-4e6b-89ff-55cd4e854207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "def serialize(filename, obj):\n",
    "    with tf.io.gfile.GFile(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "        \n",
    "def save_word_vocab_embeddings(output_path, word_vocab, word_embeddings_matrix):\n",
    "    to_serialize = (word_vocab, word_embeddings_matrix)\n",
    "    serialize(output_path, to_serialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b917e-8748-4485-bc5b-b15c6cbdfa1d",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa348419-acd7-442b-b1eb-b41e5d6d8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word frequencies...\n",
      "Number of vocabulary in Title (raw): 19929\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_title = [eval(t) for t in job_df_30['Title_tokenized'].values.tolist()]\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_title = get_words_freq(tokenized_texts_title)\n",
    "print('Number of vocabulary in {} (raw): {}'.format('Title', len(words_freq_title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438aef2-ac3c-4bcb-9ae1-92b875186922",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab_title, word_embeddings_matrix_title = google_process_word_embedding_for_corpus_vocab(w2v_model, words_freq_title, 100000)\n",
    "print('Saving word embeddings and vocab.: {}'.format('../language_models/pickles/google_word_vocab_embeddings_14d_30_Title_consider_user.pickle'))\n",
    "save_word_vocab_embeddings('../language_models/pickles/google_word_vocab_embeddings_14d_30_Title_consider_user.pickle', word_vocab_title, word_embeddings_matrix_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc41ab5f-89c0-44b1-a656-6a0874b8f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word_vocab_embeddings('../language_models/pickles/google_word_vocab_embeddings_14d_30_Title_consider_user.pickle', word_vocab_title, word_embeddings_matrix_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9245832-35e9-47e9-bbe3-4c2c362a685c",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "483d79eb-220d-4f01-a224-0136ab89f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word frequencies...\n",
      "Number of vocabulary in All (raw): 820935\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_all = [eval(t) for t in job_df_30['All_tokenized'].values.tolist()]\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_all = get_words_freq(tokenized_texts_all)\n",
    "print('Number of vocabulary in {} (raw): {}'.format('All', len(words_freq_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cce973-3b54-4b24-aaa2-0c23a655ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens vocab. from texts: 820935\n",
      "Tokens vocab. from texts with freq > 1: 97696\n",
      "Most common tokens vocab. from texts: 97696\n"
     ]
    }
   ],
   "source": [
    "word_vocab_all, word_embeddings_matrix_all = google_process_word_embedding_for_corpus_vocab(w2v_model, words_freq_all, 100000)\n",
    "print('Saving word embeddings and vocab.: {}'.format('../language_models/pickles/google_word_vocab_embeddings_14d_30_All_consider_user.pickle'))\n",
    "save_word_vocab_embeddings('../language_models/pickles/google_word_vocab_embeddings_14d_30_All_consider_user.pickle', word_vocab_all, word_embeddings_matrix_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce83e4-7cbc-4a7c-9a62-445f8f78e10a",
   "metadata": {},
   "source": [
    "# Step 4: Convert tokens into int numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c438de1-a3f5-49f5-92a5-6cf5ff04c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting tokens to int numbers (according to the vocab.)...')\n",
    "texts_int_title, texts_lengths_title = convert_tokens_to_int(tokenized_texts_title, word_vocab_titletijob_df_30)\n",
    "job_df_30['Title_length'] = texts_lengths_title\n",
    "job_df_30['Title_int'] = texts_int_title\n",
    "\n",
    "texts_int_title, texts_lengths_title = convert_tokens_to_int(tokenized_texts_title, word_vocab_titletijob_df_30)\n",
    "job_df_30['Title_length'] = texts_lengths_title\n",
    "job_df_30['Title_int'] = texts_int_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ad2de-541d-47b0-aba7-4bcb74420048",
   "metadata": {},
   "source": [
    "# Step 5: Export to tf and df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96231923-2fff-4e25-a738-7e3495860eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_export_df = news_df[['id', 'url', #For debug\n",
    "                                'id_encoded', \n",
    "                                'category0_encoded',\n",
    "                                'category1_encoded',\n",
    "                                'keywords_encoded',\n",
    "                                'author_encoded',\n",
    "                                'concepts_encoded',\n",
    "                                'entities_encoded',\n",
    "                                'locations_encoded',\n",
    "                                'persons_encoded',\n",
    "                                'created_at_ts',\n",
    "                                'text_length', \n",
    "                                'text_int']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
