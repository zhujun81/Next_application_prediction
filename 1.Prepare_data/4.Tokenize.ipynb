{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e19e040-a5e4-4bb6-a709-36e02eb4de45",
   "metadata": {},
   "source": [
    "Tokenize content and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed53755-61be-4e1b-b36c-87a048fbf432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b182a0e6-31c5-48d1-ab6c-e17970ded3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../data/\"\n",
    "dataset = \"cb12/\"\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab435f-149b-4d90-a67e-f7ba2069740d",
   "metadata": {},
   "source": [
    "# Step 1: Load job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1204ae-ee95-42bb-939d-35b43761a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading job from file: ../data/cb12/processed/jobs_14d_30_consider_user_encoded.csv\n",
      "Job data shape:  (207972, 23)\n",
      "Unique JobCity:  5744\n",
      "Unique JobState:  54\n",
      "Unique JobCountry:  3\n"
     ]
    }
   ],
   "source": [
    "print('Loading job from file: {}'.format(processed_path + 'jobs_14d_30_consider_user_encoded.csv'))\n",
    "job_df_30 = pd.read_csv(processed_path + 'jobs_14d_30_consider_user_encoded.csv', header=0, sep='\\t')\n",
    "print('Job data shape: ', job_df_30.shape)\n",
    "print('Unique JobCity: ', len(job_df_30.JobCity.unique()))\n",
    "print('Unique JobState: ', len(job_df_30.JobState.unique()))\n",
    "print('Unique JobCountry: ', len(job_df_30.JobCountry.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8518ac49-49b2-44d9-984c-ae3526c7f177",
   "metadata": {},
   "source": [
    "# Step 2: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813894f9-5aed-4468-aa00-bf12e70e6309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts and Converting tokens to int numbers...\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing texts and Converting tokens to int numbers...')\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits, punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "def tokenize_text(text, clean_str_fn):\n",
    "    text = clean_str_fn(text)\n",
    "    tokenized_text = []\n",
    "    if type(text) != float:\n",
    "        text = text.strip(' ')\n",
    "        remove_punctuations = str.maketrans('', '', punctuation)\n",
    "        text = text.translate(remove_punctuations)\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        text = text.translate(remove_digits)\n",
    "        tokens = text.split(' ')\n",
    "        for token in tokens:\n",
    "            if token and token not in stopwords.words('english'):\n",
    "                tokenized_text.append(token)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenization_fn=None, clean_str_fn=lambda x: x):\n",
    "    if tokenization_fn == None:\n",
    "        tokenized_texts = [tokenize_text(text, clean_str_fn) for text in texts]\n",
    "    else:\n",
    "        tokenized_texts = [tokenization_fn(text) for text in texts]\n",
    "\n",
    "    return tokenized_texts\n",
    "\n",
    "\n",
    "def get_words_freq(tokenized_texts):\n",
    "    words_freq = FreqDist([word for text in tokenized_texts for word in text])\n",
    "    return words_freq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0a66c-7b0a-4ae6-b1b5-46dfefb3565f",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239df58c-e006-4f0a-82e8-df80fa4864d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n",
      "Computing word frequencies...\n",
      "Number of vocabulary in Title (raw): 19929\n"
     ]
    }
   ],
   "source": [
    "content_type = 'Title'\n",
    "print('Tokenizing texts...')\n",
    "tokenized_texts_title = tokenize_texts(job_df_30[content_type].values)\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_title = get_words_freq(tokenized_texts_title)\n",
    "print('Number of vocabulary in {} (raw): {}'.format(content_type, len(words_freq_title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "063c1614-88bc-4880-9886-c1b113cccf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_30['Title_tokenized'] = tokenized_texts_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2ddfa-60ee-4d18-bcb1-c095b5392994",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbc5755-04c5-4e4e-a0f9-065daf15b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n",
      "Computing word frequencies...\n",
      "Number of vocabulary in Description (raw): 567788\n"
     ]
    }
   ],
   "source": [
    "content_type = 'Description'\n",
    "print('Tokenizing texts...')\n",
    "tokenized_texts_description = tokenize_texts(job_df_30[content_type].values)\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_description = get_words_freq(tokenized_texts_description)\n",
    "print('Number of vocabulary in {} (raw): {}'.format(content_type, len(words_freq_description)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1825a145-e880-400c-b2bd-f0173dc76ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_30['Description_tokenized'] = tokenized_texts_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56902e-ec9d-4523-aac6-2a79cdf75e65",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42add54-544e-45e8-bc37-69e8728d9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n",
      "Computing word frequencies...\n",
      "Number of vocabulary in Requirements (raw): 275779\n"
     ]
    }
   ],
   "source": [
    "content_type = 'Requirements'\n",
    "print('Tokenizing texts...')\n",
    "tokenized_texts_requirements = tokenize_texts(job_df_30[content_type].values)\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_requirements = get_words_freq(tokenized_texts_requirements)\n",
    "print('Number of vocabulary in {} (raw): {}'.format(content_type, len(words_freq_requirements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea188819-23de-4de5-b36f-393b3bc6db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_30['Requirements_tokenized'] = tokenized_texts_requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7075bce-fbe4-4463-852e-4df33d7903f9",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d48831-56f2-4535-91e1-97e6bafd4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n",
      "Computing word frequencies...\n",
      "Number of vocabulary in All (raw): 207972\n"
     ]
    }
   ],
   "source": [
    "content_type = 'All'\n",
    "print('Tokenizing texts...')\n",
    "tokenized_texts_all = tokenize_texts(job_df_30[content_type].values)\n",
    "print('Computing word frequencies...')\n",
    "# A dictionary \n",
    "words_freq_all = get_words_freq(tokenized_texts_all)\n",
    "print('Number of vocabulary in {} (raw): {}'.format(content_type, len(tokenized_texts_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44558bd0-cde2-412d-82d0-e8fa0c46ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_30['All_tokenized'] = tokenized_texts_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c076e8c-3f0e-4533-91c2-68fa73bf47d9",
   "metadata": {},
   "source": [
    "# Step 3: Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8664484d-4da4-4b9e-9933-f38fd13812ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_30.to_csv(processed_path + 'jobs_14d_30_consider_user_encoded_tokenized.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccd14f-becb-439a-bace-cacbf3364031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
