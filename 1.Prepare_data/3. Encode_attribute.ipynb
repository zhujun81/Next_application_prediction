{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21203751-6f09-4293-83f9-3513cddd30a9",
   "metadata": {},
   "source": [
    "Encode each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec460110-d656-4296-bad4-71f3e100696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7299d86-8a0f-47e5-9559-1417f07ae5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b80eef5-2e16-43ac-b7eb-c2f1c7a90d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../data/\"\n",
    "dataset = \"cb12/\"\n",
    "\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7ac1e-693c-4da1-8d46-5001f1d6c629",
   "metadata": {},
   "source": [
    "# Step 1: User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baafbed7-548d-4595-b520-910e2afb1b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111785, 15)\n",
      "Unique UserCity:  6701\n",
      "Unique UserState:  121\n",
      "Unique UserCountry:  33\n",
      "Unique UserDegree:  7\n",
      "Unique UserMajor:  21224\n"
     ]
    }
   ],
   "source": [
    "user_df_30 = pd.read_csv(processed_path + 'users_14d_30_consider_user.csv', sep='\\t')\n",
    "print(user_df_30.shape)\n",
    "print('Unique UserCity: ', len(user_df_30.UserCity.unique()))\n",
    "print('Unique UserState: ', len(user_df_30.UserState.unique()))\n",
    "print('Unique UserCountry: ', len(user_df_30.UserCountry.unique()))\n",
    "print('Unique UserDegree: ', len(user_df_30.UserDegree.unique()))\n",
    "print('Unique UserMajor: ', len(user_df_30.UserMajor.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19b205-f444-4ac6-81ee-376b19ad1944",
   "metadata": {},
   "source": [
    "# Step 2: Job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075718c0-2db2-40bd-a2bf-fb47459a4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading job from file: ../data/cb12/processed/job_14d_30_consider_user.csv\n",
      "Job data shape:  (207972, 19)\n",
      "Unique JobCity:  5744\n",
      "Unique JobState:  54\n",
      "Unique JobCountry:  3\n"
     ]
    }
   ],
   "source": [
    "print('Loading job from file: {}'.format(processed_path + 'job_14d_30_consider_user.csv'))\n",
    "job_df_30 = pd.read_csv(processed_path + 'jobs_14d_30_consider_user.csv', header=0, sep='\\t')\n",
    "print('Job data shape: ', job_df_30.shape)\n",
    "print('Unique JobCity: ', len(job_df_30.JobCity.unique()))\n",
    "print('Unique JobState: ', len(job_df_30.JobState.unique()))\n",
    "print('Unique JobCountry: ', len(job_df_30.JobCountry.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253834a0-92a4-4693-9e60-19e0bf88413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserCity:  6701\n",
      "JobCity:  5744\n",
      "TotalCity:  8226\n"
     ]
    }
   ],
   "source": [
    "list_UserCity_30 = user_df_30.UserCity.unique().tolist()\n",
    "print('UserCity: ', len(list_UserCity_30))\n",
    "list_JobCity_30 = job_df_30.JobCity.unique().tolist()\n",
    "print('JobCity: ', len(list_JobCity_30))\n",
    "list_City_30 = list(set(list_UserCity_30) | set(list_JobCity_30))\n",
    "print('TotalCity: ', len(list_City_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdc2fdd-98c7-4ebf-88b0-8ed99c01cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserState:  121\n",
      "JobState:  54\n",
      "TotalState:  122\n"
     ]
    }
   ],
   "source": [
    "list_UserState_30 = user_df_30.UserState.unique().tolist()\n",
    "print('UserState: ', len(list_UserState_30))\n",
    "list_JobState_30 = job_df_30.JobState.unique().tolist()\n",
    "print('JobState: ', len(list_JobState_30))\n",
    "list_State_30 = list(set(list_UserState_30) | set(list_JobState_30))\n",
    "print('TotalState: ', len(list_State_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89614f4-72bd-4181-9568-e4d836d60a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserCountry:  33\n",
      "JobCountry:  3\n",
      "TotalCountry:  33\n"
     ]
    }
   ],
   "source": [
    "list_UserCountry_30 = user_df_30.UserCountry.unique().tolist()\n",
    "print('UserCountry: ', len(list_UserCountry_30))\n",
    "list_JobCountry_30 = job_df_30.JobCountry.unique().tolist()\n",
    "print('JobCountry: ', len(list_JobCountry_30))\n",
    "list_Country_30 = list(set(list_UserCountry_30) | set(list_JobCountry_30))\n",
    "print('TotalCountry: ', len(list_Country_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7620b80d-142e-4766-8185-ec2147c29275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categ_feature(value, encoder_dict):\n",
    "    if value in encoder_dict:\n",
    "        return encoder_dict[value]\n",
    "    else:\n",
    "        return encoder_dict[get_unfrequent_token()]\n",
    "\n",
    "\n",
    "def transform_categorical_column(series, encoder):\n",
    "    return series.apply(lambda x: encode_categ_feature(x, encoder)) \n",
    "\n",
    "\n",
    "def get_pad_token():\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    return PAD_TOKEN\n",
    "\n",
    "def get_unfrequent_token():\n",
    "    UNFREQ_TOKEN = '<UNF>'\n",
    "    return UNFREQ_TOKEN\n",
    "\n",
    "def get_categ_encoder_from_values(values, include_pad_token=True, include_unfrequent_token=False):\n",
    "    encoder_values = []\n",
    "    if include_pad_token:\n",
    "        encoder_values.append(get_pad_token())\n",
    "    if include_unfrequent_token:\n",
    "        encoder_values.append(get_unfrequent_token())\n",
    "    encoder_values.extend(values)\n",
    "    encoder_ids = list(range(len(encoder_values)))\n",
    "    encoder_dict = dict(zip(encoder_values, encoder_ids))\n",
    "    return encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbde8ba1-133c-485b-9213-363bc64dc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share between Job and User\n",
    "City_encoder_30 = get_categ_encoder_from_values(list_City_30)\n",
    "State_encoder_30 = get_categ_encoder_from_values(list_State_30)\n",
    "Country_encoder_30 = get_categ_encoder_from_values(list_Country_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f420a-ddc0-48ae-a9f8-ee6978a65079",
   "metadata": {},
   "source": [
    "# Step 3: Process job features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d652e00-e541-461b-96d3-bc3431c83c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Job attributes ...\n",
      "Unique Job 207973\n",
      "Unique City 8227\n",
      "Unique State 123\n",
      "Unique Country 34\n"
     ]
    }
   ],
   "source": [
    "print('Encoding Job attributes ...')\n",
    "\n",
    "\n",
    "def process_job_features(data_df, City_encoder, State_encoder, Country_encoder):\n",
    "    JobID_encoder = get_categ_encoder_from_values(data_df['JobID'])\n",
    "    print('Unique Job {}'.format(len(JobID_encoder)))\n",
    "    data_df['JobID_encoded'] = transform_categorical_column(data_df['JobID'], JobID_encoder)\n",
    "    \n",
    "    print('Unique City {}'.format(len(City_encoder)))\n",
    "    data_df['JobCity_encoded'] = transform_categorical_column(data_df['JobCity'], City_encoder)\n",
    "   \n",
    "    print('Unique State {}'.format(len(State_encoder)))\n",
    "    data_df['JobState_encoded'] = transform_categorical_column(data_df['JobState'], State_encoder)\n",
    "    \n",
    "    print('Unique Country {}'.format(len(Country_encoder)))\n",
    "    data_df['JobCountry_encoded'] = transform_categorical_column(data_df['JobCountry'], Country_encoder)\n",
    "    \n",
    "    job_features_encoders = {\n",
    "        'JobID': JobID_encoder, \n",
    "        'JobCity': City_encoder, \n",
    "        'JobState': State_encoder,\n",
    "        'JobCountry': Country_encoder\n",
    "    }\n",
    "    return job_features_encoders, data_df\n",
    "  \n",
    "job_features_encoders_30, job_df_encoded_30 = process_job_features(job_df_30, City_encoder_30, State_encoder_30, Country_encoder_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c0f7586-29b9-4cf3-bda3-94a43c2312ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving job feature encoder...\n"
     ]
    }
   ],
   "source": [
    "def serialize(filename, obj):\n",
    "    with tf.io.gfile.GFile(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "        \n",
    "def save_job_feature_encoders(output_path, job_features_encoders):\n",
    "    to_serialize = (job_features_encoders)\n",
    "    serialize(output_path, to_serialize)\n",
    "\n",
    "print('Saving job feature encoder...')\n",
    "save_job_feature_encoders(processed_path + 'job_feature_encoders_14d_30_consider_user.pickle', job_features_encoders_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca34461d-ca18-4581-b481-6663eae6e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df_encoded_30.to_csv(processed_path + 'jobs_14d_30_consider_user_encoded.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d627c-aea2-454d-8cde-4f76df715dce",
   "metadata": {},
   "source": [
    "# Step 4: Process user features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e80a876-f183-4321-a922-1455a5a606d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique User 111786\n",
      "Unique City 8227\n",
      "Unique State 123\n",
      "Unique Country 34\n",
      "Unique UserDegree 8\n",
      "Unique UserMajor 21225\n"
     ]
    }
   ],
   "source": [
    "def process_user_features(data_df, City_encoder, State_encoder, Country_encoder):\n",
    "    UserID_encoder = get_categ_encoder_from_values(data_df['UserID'])\n",
    "    print('Unique User {}'.format(len(UserID_encoder)))\n",
    "    data_df['UserID_encoded'] = transform_categorical_column(data_df['UserID'], UserID_encoder)\n",
    "   \n",
    "    print('Unique City {}'.format(len(City_encoder)))\n",
    "    data_df['UserCity_encoded'] = transform_categorical_column(data_df['UserCity'], City_encoder)\n",
    "   \n",
    "    print('Unique State {}'.format(len(State_encoder)))\n",
    "    data_df['UserState_encoded'] = transform_categorical_column(data_df['UserState'], State_encoder)\n",
    "    \n",
    "    print('Unique Country {}'.format(len(Country_encoder)))\n",
    "    data_df['UserCountry_encoded'] = transform_categorical_column(data_df['UserCountry'], Country_encoder)\n",
    "\n",
    "    UserDegree_encoder = get_categ_encoder_from_values(data_df['UserDegree'].unique())\n",
    "    print('Unique UserDegree {}'.format(len(UserDegree_encoder)))\n",
    "    data_df['UserDegree_encoded'] = transform_categorical_column(data_df['UserDegree'], UserDegree_encoder)\n",
    "     \n",
    "    UserMajor_encoder = get_categ_encoder_from_values(data_df['UserMajor'].unique())\n",
    "    print('Unique UserMajor {}'.format(len(UserMajor_encoder )))\n",
    "    data_df['UserMajor_encoded'] = transform_categorical_column(data_df['UserMajor'], UserMajor_encoder)\n",
    "  \n",
    "    user_features_encoders = {\n",
    "        'UserID': UserID_encoder, \n",
    "        'UserCity': City_encoder, \n",
    "        'UserState': State_encoder,\n",
    "        'UserCountry': Country_encoder,\n",
    "        'UserDegree': UserDegree_encoder,\n",
    "        'UserMajor': UserMajor_encoder\n",
    "    }\n",
    "    return user_features_encoders, data_df\n",
    "  \n",
    "user_features_encoders_30, user_df_encoded_30 = process_user_features(user_df_30, City_encoder_30, State_encoder_30, Country_encoder_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cba13a3-80bb-4f86-ae17-cb30cda7c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving user feature encoder...\n"
     ]
    }
   ],
   "source": [
    "def save_user_feature_encoders(output_path, user_features_encoders):\n",
    "    to_serialize = (user_features_encoders)\n",
    "    serialize(output_path, to_serialize)\n",
    "\n",
    "print('Saving user feature encoder...')\n",
    "save_user_feature_encoders(processed_path + 'user_feature_encoders_14d_30_consider_user.pickle', user_features_encoders_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b8f2df-9f69-4208-a363-df447490337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_encoded_30.to_csv(processed_path + 'users_14d_30_consider_user_encoded.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
